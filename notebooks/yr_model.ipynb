{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ff4e8bf4",
   "metadata": {},
   "source": [
    "##### setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e0e9961",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "import xgboost as xgb\n",
    "import numpy as np\n",
    "\n",
    "def preprocess_turnover_data(turnover_trend):\n",
    "    turnover_trend['year'] = pd.to_datetime(turnover_trend['recorddate_key']).dt.to_period('Y')\n",
    "\n",
    "    turnover_trend.columns = [col.lower() for col in turnover_trend.columns]\n",
    "    # get dummies for all categoricals, drop originals\n",
    "    categorical_features = ['job_title', 'gender_short', 'termreason_desc', 'termtype_desc', 'business_unit',  'department_name']\n",
    "    turnover_trend = pd.get_dummies(turnover_trend, columns=categorical_features, drop_first=False)\n",
    "    turnover_trend.columns = [col.lower() for col in turnover_trend.columns]\n",
    "\n",
    "    # 1 = voluntary turnover, 0 = stayed\n",
    "    turnover_trend['voluntary_turnover'] = (turnover_trend['termreason_desc_resignaton'] == True).astype(int)\n",
    "\n",
    "    # drop any unnecessary columns\n",
    "    i=0\n",
    "    drop_cols = ['termreason_desc_layoff',\n",
    "                'termreason_desc_not applicable',\n",
    "                'termreason_desc_resignaton',\n",
    "                'termreason_desc_retirement',\n",
    "                'termtype_desc_involuntary',\n",
    "                'termtype_desc_not applicable',\n",
    "                'termtype_desc_voluntary']\n",
    "\n",
    "    for each in turnover_trend.dtypes:\n",
    "        if each == 'object':\n",
    "            drop_cols.append(turnover_trend.columns[i])\n",
    "        i+=1\n",
    "\n",
    "    # remove potential model by columns from drop_cols\n",
    "    model_by_cols = ['city_name', 'store_name']\n",
    "    drop_cols = [col for col in drop_cols if col not in model_by_cols]\n",
    "    turnover_trend.drop(columns=drop_cols+['employeeid','status_year'], inplace=True)\n",
    "    \n",
    "    return turnover_trend\n",
    "\n",
    "def yearify_features(df):\n",
    "    \"\"\"Convert features to yearly averages or sums as appropriate.\"\"\"\n",
    "    df = df.copy()\n",
    "    df.drop(columns=['city_name', 'store_name'], inplace=True)\n",
    "    df = df.groupby('year').mean(numeric_only=False)\n",
    "    df['voluntary_turnover'] = df['voluntary_turnover']*100\n",
    "    return df\n",
    "\n",
    "def city_store_yearify(df):\n",
    "    \"\"\"Convert features to yearly averages or sums as appropriate.\"\"\"\n",
    "    df = df.copy()\n",
    "    df = df.groupby(['year', 'city_name', 'store_name']).mean(numeric_only=False)\n",
    "    df.reset_index(inplace=True)\n",
    "    df.set_index(['year'], inplace=True)\n",
    "    df['voluntary_turnover'] = df['voluntary_turnover']*100\n",
    "    return df\n",
    "\n",
    "def load_and_process_data():\n",
    "    turnover_trend = pd.read_csv(\"../data/10yr_turnover.csv\")\n",
    "    turnover_trend.columns = [col.lower() for col in turnover_trend.columns]\n",
    "    economic_data = pd.read_excel(\"../data/economic_data.xlsx\", sheet_name='annual_data')\n",
    "    turnover_trend = turnover_trend.merge(economic_data, how='left', left_on='status_year', right_on='year')\n",
    "\n",
    "    preprocessed_turnover_trend = preprocess_turnover_data(turnover_trend)\n",
    "    year_df = yearify_features(preprocessed_turnover_trend)\n",
    "    city_store_yr_df = city_store_yearify(preprocessed_turnover_trend)\n",
    "\n",
    "    city_store_yr_df = pd.get_dummies(city_store_yr_df, columns=['city_name', 'store_name'], drop_first=False)\n",
    "\n",
    "    # Fix column names to remove special characters that cause issues with LightGBM\n",
    "    city_store_yr_df.columns = city_store_yr_df.columns.str.replace('[^A-Za-z0-9_]', '_', regex=True)\n",
    "    city_store_yr_df.columns = city_store_yr_df.columns.str.replace('__+', '_', regex=True)\n",
    "    city_store_yr_df.columns = city_store_yr_df.columns.str.strip('_')\n",
    "\n",
    "    X = city_store_yr_df.drop(['voluntary_turnover'], axis=1)\n",
    "    y = city_store_yr_df['voluntary_turnover']  # Use actual percentage values\n",
    "    return X, y\n",
    "\n",
    "def select_top_features(X, y):\n",
    "    X = X.copy()\n",
    "    from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "    keep_cols = []\n",
    "    for col in X.columns:\n",
    "        if 'city_name' in col:\n",
    "            keep_cols.append(col)\n",
    "\n",
    "    rf = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "    rf.fit(X, y)\n",
    "    feature_importance = pd.DataFrame({\n",
    "        'feature': X.columns,\n",
    "        'importance': rf.feature_importances_\n",
    "    }).sort_values('importance', ascending=False)\n",
    "\n",
    "    top_features = feature_importance.head(30)['feature'].tolist()\n",
    "    columns_to_keep = set(keep_cols).union(set(top_features))\n",
    "    X = X[list(columns_to_keep)]\n",
    "    return X\n",
    "def fit_model(X, y):\n",
    "    import lightgbm as lgb\n",
    "    from sklearn.model_selection import GridSearchCV\n",
    "    from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    # Define hyperparameter grid for LightGBM\n",
    "    param_grid = {\n",
    "        'n_estimators': [50, 100],\n",
    "        'max_depth': [2,3],\n",
    "        'learning_rate': [0.05, 0.1],\n",
    "        'num_leaves': [15, 30],\n",
    "        'subsample': [0.8, 1.0],\n",
    "        'colsample_bytree': [0.8, 1.0],\n",
    "        'reg_alpha': [0, 0.1, 0.5],\n",
    "        'reg_lambda': [0.1, 1.0, 2.0]\n",
    "    }\n",
    "\n",
    "    # Create LightGBM regressor\n",
    "    lgb_model = lgb.LGBMRegressor(random_state=42, verbose=-1)\n",
    "\n",
    "    # Perform grid search with cross-validation\n",
    "    grid_search = GridSearchCV(\n",
    "        estimator=lgb_model,\n",
    "        param_grid=param_grid,\n",
    "        cv=5,\n",
    "        scoring='r2',\n",
    "        n_jobs=-1,\n",
    "        verbose=1\n",
    "    )\n",
    "\n",
    "    # Fit the grid search\n",
    "    grid_search.fit(X_train, y_train)\n",
    "    best_lgbm_model = grid_search.best_estimator_\n",
    "\n",
    "    # Make predictions\n",
    "    y_pred = best_lgbm_model.predict(X_test)\n",
    "\n",
    "    # Calculate metrics\n",
    "    train_score = best_lgbm_model.score(X_train, y_train)\n",
    "    test_score = r2_score(y_test, y_pred)\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    rmse = np.sqrt(mse)\n",
    "\n",
    "    print(\"Best LightGBM Model Results:\")\n",
    "    print(f\"Best parameters: {grid_search.best_params_}\")\n",
    "    print(f\"Best CV R² score: {grid_search.best_score_:.4f}\")\n",
    "    print(f\"Training R²: {train_score:.4f}\")\n",
    "    print(f\"Test R²: {test_score:.4f}\")\n",
    "    print(f\"Test MSE: {mse:.4f}\")\n",
    "    print(f\"Test RMSE: {rmse:.4f}\")\n",
    "    print(f\"Test MAE: {mae:.4f}\")\n",
    "    print(f\"R² difference (train - test): {train_score - test_score:.4f}\")\n",
    "\n",
    "    if abs(train_score - test_score) > 0.1:\n",
    "        print(\"Warning: Potential overfitting detected!\")\n",
    "    else:\n",
    "        print(\"Model appears to generalize well.\")\n",
    "    \n",
    "    return best_lgbm_model\n",
    "\n",
    "def get_shap(best_lgbm_model, X):\n",
    "    # import shap\n",
    "    import shap\n",
    "    # Initialize SHAP explainer\n",
    "    explainer = shap.TreeExplainer(best_lgbm_model)\n",
    "    shap_values = explainer.shap_values(X)\n",
    "    shap_values = pd.DataFrame(shap_values, columns=X.columns)\n",
    "    shap.summary_plot(shap_values.values, X, plot_type=\"bar\", max_display=10)\n",
    "    shap.summary_plot(shap_values.values, X)\n",
    "    return shap_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50818718",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ce7ddafa",
   "metadata": {},
   "source": [
    "# city / store / year model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d83d374b",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = load_and_process_data()\n",
    "X = select_top_features(X, y)\n",
    "best_lgbm_model = fit_model(X, y)\n",
    "\n",
    "shap_values = get_shap(best_lgbm_model, X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "341ce04f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's try several approaches to reduce overfitting\n",
    "\n",
    "# 1. Scale the features first - KNN is sensitive to feature scales\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# 2. Use much higher k values and simpler approach\n",
    "param_grid_improved = {\n",
    "    'n_neighbors': [50, 75, 100, 150],  # Much higher k values\n",
    "    'weights': ['uniform'],  # Only uniform weights to reduce complexity\n",
    "    'metric': ['euclidean']  # Only euclidean distance\n",
    "}\n",
    "\n",
    "# 3. Create new KNN with regularization-like effect\n",
    "knn_improved = KNeighborsRegressor()\n",
    "\n",
    "grid_search_improved = GridSearchCV(\n",
    "    estimator=knn_improved,\n",
    "    param_grid=param_grid_improved,\n",
    "    cv=10,  # More folds for better validation\n",
    "    scoring='r2',\n",
    "    n_jobs=-1,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Fit with scaled data\n",
    "grid_search_improved.fit(X_train_scaled, y_train)\n",
    "best_knn_improved = grid_search_improved.best_estimator_\n",
    "\n",
    "# Make predictions\n",
    "y_pred_improved = best_knn_improved.predict(X_test_scaled)\n",
    "\n",
    "# Calculate metrics\n",
    "train_score_improved = best_knn_improved.score(X_train_scaled, y_train)\n",
    "test_score_improved = r2_score(y_test, y_pred_improved)\n",
    "mse_improved = mean_squared_error(y_test, y_pred_improved)\n",
    "mae_improved = mean_absolute_error(y_test, y_pred_improved)\n",
    "rmse_improved = np.sqrt(mse_improved)\n",
    "\n",
    "print(\"Improved KNN Model Results:\")\n",
    "print(f\"Best parameters: {grid_search_improved.best_params_}\")\n",
    "print(f\"Best CV R² score: {grid_search_improved.best_score_:.4f}\")\n",
    "print(f\"Training R²: {train_score_improved:.4f}\")\n",
    "print(f\"Test R²: {test_score_improved:.4f}\")\n",
    "print(f\"Test MSE: {mse_improved:.4f}\")\n",
    "print(f\"Test RMSE: {rmse_improved:.4f}\")\n",
    "print(f\"Test MAE: {mae_improved:.4f}\")\n",
    "print(f\"R² difference (train - test): {train_score_improved - test_score_improved:.4f}\")\n",
    "\n",
    "if abs(train_score_improved - test_score_improved) > 0.1:\n",
    "    print(\"Warning: Still overfitting detected!\")\n",
    "else:\n",
    "    print(\"Model generalization improved.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "turnover",
   "language": "python",
   "name": "turnover"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
